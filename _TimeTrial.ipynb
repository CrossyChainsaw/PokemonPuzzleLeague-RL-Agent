{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from PIL import ImageGrab, Image\n",
    "import pyautogui\n",
    "import win32gui\n",
    "import cv2\n",
    "import win32con\n",
    "from torchvision import transforms\n",
    "from collections import deque\n",
    "import keyboard\n",
    "import pyautogui\n",
    "import ctypes\n",
    "from modules.read_memory import get_process_id, read_memory, PROCESS_ALL_ACCESS\n",
    "\n",
    "P1_CELL_BBOX_DOLPHIN = (390, 160, 845, 960)\n",
    "\n",
    "class PPLEnv(gym.Env):\n",
    "    def __init__(self, base_address, color_mode='grey', cursor_logging=False, screenshot_saving=False, load_state_key='f2'):\n",
    "        super(PPLEnv, self).__init__()\n",
    "        # Action Space\n",
    "        self.action_space = spaces.Discrete(5)  # 6 possible actions: W, A, S, D, H, K\n",
    "\n",
    "        # Image Size\n",
    "        self.img_size_color = (32, 32)\n",
    "        \n",
    "        # Logging\n",
    "        self.cursor_logging = cursor_logging\n",
    "        self.save_screenshots = screenshot_saving\n",
    "        self.screenshot_history = []\n",
    "        self.preprocessed_screenshot_history = []\n",
    "\n",
    "        # Bounding boxes \n",
    "        self.state_bbox = P1_CELL_BBOX_DOLPHIN # bounding box for state\n",
    "        self.bbox_bottom_left_cell_spotlight = (400, 965, 440, 1005) # bounding box for check game over\n",
    "\n",
    "        # Keys\n",
    "        self.load_state_key = load_state_key\n",
    "\n",
    "        # Template Images\n",
    "        self.template_cell_gray = cv2.cvtColor(cv2.imread(\"images/temp_img_cell.png\"), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Track episode score for calc reward\n",
    "        self.episode_score = 0\n",
    "        self.punishment = -250\n",
    "        self.no_reward_streak = 0\n",
    "\n",
    "        # Select preprocessing function based on the mode\n",
    "        if color_mode == 'grey':\n",
    "            self.preprocess_frame = preprocess_frame_grey\n",
    "        elif color_mode == 'color':\n",
    "            self.preprocess_frame = preprocess_frame_color\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode specified. Use 'grey' or 'color'.\")\n",
    "        \n",
    "        # Find pointers (use QtGui.dll as base adres)\n",
    "        process_name = \"DolphinMemoryEngine.exe\"\n",
    "        process_id = get_process_id(process_name)\n",
    "        self.process_handle = ctypes.windll.kernel32.OpenProcess(PROCESS_ALL_ACCESS, False, process_id)\n",
    "        \n",
    "        score_base_offset=0x006EA800\n",
    "        score_offsets=[0x130, 0x218, 0x18, 0x40, 0x220, 0xAD0]\n",
    "        self.score_pointer = get_pointer(self, base_address, score_base_offset, score_offsets)\n",
    "\n",
    "        cursor_horizontal_base_offset = 0x006EB0F8\n",
    "        cursor_horizontal_offsets = [0x98, 0x8, 0x1C8, 0x128, 0x8, 0x0, 0x8C0]\n",
    "        self.cursor_horizontal_pointer = get_pointer(self, base_address, cursor_horizontal_base_offset, offsets=cursor_horizontal_offsets)\n",
    "\n",
    "        cursor_vertical_base_offset = 0x006EB2A8\n",
    "        cursor_vertical_offsets = [0x78, 0x1C0, 0x188, 0xE0, 0x40]\n",
    "        self.cursor_vertical_pointer = get_pointer(self, base_address, cursor_vertical_base_offset, offsets=cursor_vertical_offsets)\n",
    "\n",
    "        # Test memory pointers\n",
    "        cursor_x, cursor_y = get_cursor_pos(self)\n",
    "        score = get_score(self)\n",
    "        print(f\"Environment succesfully initialized \\n\" + f\"- cursor: ({cursor_x}, {cursor_y})\\n\" + f\"- score: {score}\")\n",
    "        \n",
    "    # FUNCTIONS\n",
    "    def step(self, action):\n",
    "        do_action(action=action)                    \n",
    "        observation = self.get_state()              \n",
    "        done = self.is_game_over()                  \n",
    "        reward = self.calculate_reward(done)            \n",
    "        return observation, reward, done\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Restart from save state\n",
    "        keyboard.press(self.load_state_key) # load save state in slot f2\n",
    "        time.sleep(.025) # wait to release key\n",
    "        keyboard.release(self.load_state_key)\n",
    "        time.sleep(2.2) # wait for dolphin to update\n",
    "        \n",
    "        # Reset Variables\n",
    "        self.episode_score = 0 # reset total episode reward\n",
    "        self.preprocessed_screenshot_history = [] # resetting history to prevent extremely large arrays of images\n",
    "        self.no_reward_streak = 0\n",
    "\n",
    "        # Return State\n",
    "        observation = self.get_state()\n",
    "        return observation\n",
    "\n",
    "    def calculate_reward(self, done):\n",
    "        if done:\n",
    "            return self.punishment\n",
    "        else:            \n",
    "            # Get score from the game by reading memory\n",
    "            total_score = get_score(self)\n",
    "            # calculate delta score (score gain)\n",
    "            reward = total_score - self.episode_score\n",
    "            # keep track of total game score\n",
    "            self.episode_score+=reward\n",
    "            if reward == 0:\n",
    "                self.no_reward_streak+=1\n",
    "                return reward - self.no_reward_streak # everytime punish 1 more\n",
    "            else:\n",
    "                self.no_reward_streak=0 # reset the no reward streak\n",
    "            # multiply reward by 10 to outweigh the continous -1 \n",
    "            return reward\n",
    "\n",
    "    # should take 0.003s\n",
    "    def is_game_over(self): \n",
    "        input_img = np.array(ImageGrab.grab(bbox=self.bbox_bottom_left_cell_spotlight)) # Grab img and convert to numpy array\n",
    "        input_gray = cv2.cvtColor(input_img, cv2.COLOR_RGB2GRAY) # grayscale it\n",
    "        # Perform template matching for both templates\n",
    "        result1 = cv2.matchTemplate(input_gray, self.template_cell_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        # Get the best match positions and scores for both templates\n",
    "        _, max_val1, _, _ = cv2.minMaxLoc(result1)\n",
    "        # Accuracy Check\n",
    "        if max_val1 > 0.8:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def reset_game(self):\n",
    "        state = self.get_state()\n",
    "        return state\n",
    "    \n",
    "    def get_state(self):\n",
    "        # Grab Screenshot\n",
    "        screenshot = ImageGrab.grab(bbox=self.state_bbox)  # PIL\n",
    "        screenshot_np = np.array(screenshot)\n",
    "        screenshot_np = self.preprocess_frame(screenshot_np, self.img_size_color)\n",
    "        cursor_position_horizontal, cursor_position_vertical = get_cursor_pos(self)\n",
    "        if self.save_screenshots:\n",
    "            self.screenshot_history.append(screenshot)\n",
    "            self.preprocessed_screenshot_history.append(screenshot_np)\n",
    "        return [screenshot_np, cursor_position_horizontal, cursor_position_vertical]\n",
    "\n",
    "\n",
    "def get_cursor_pos(self):\n",
    "    cursor_horizontal = read_memory(self.process_handle, self.cursor_horizontal_pointer, data_type=ctypes.c_uint8)\n",
    "    cursor_vertical = read_memory(self.process_handle, self.cursor_vertical_pointer, data_type=ctypes.c_uint32)\n",
    "    if self.cursor_logging:\n",
    "        print(f'Cursor pos: ({cursor_horizontal}, {cursor_vertical})')\n",
    "    return cursor_horizontal, cursor_vertical\n",
    "\n",
    "def get_score(self):\n",
    "    return read_memory(self.process_handle, self.score_pointer, data_type=ctypes.c_uint32)\n",
    "\n",
    "def get_score_img():\n",
    "    score_screenshot = ImageGrab.grab(bbox=(440, 50, 510, 160)) # capture the score number\n",
    "    score_screenshot_np = np.array(score_screenshot)\n",
    "    return score_screenshot_np\n",
    "\n",
    "def do_action(action:int):\n",
    "    # Simulate key press and release for the action\n",
    "    if action == 0:\n",
    "        press_and_release('w')  # Press and release 'w'\n",
    "    elif action == 1:\n",
    "        press_and_release('a')  # Press and release 'a'\n",
    "    elif action == 2:\n",
    "        press_and_release('s')  # Press and release 's'\n",
    "    elif action == 3:\n",
    "        press_and_release('d')  # Press and release 'd'\n",
    "    elif action == 4:\n",
    "        press_and_release('h')  # Press and release 'h'\n",
    "    # elif action == 5:\n",
    "    #     press_and_release('k')  # Press and release 'k'\n",
    "\n",
    "def press_and_release(key=None):\n",
    "    keyboard.press(key)\n",
    "    time.sleep(.025)\n",
    "    keyboard.release(key)\n",
    "\n",
    "# Preprocess the frames (resize and convert to grayscale)\n",
    "def preprocess_frame_color(frame, img_size_color):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        # transforms.Grayscale(),  # Convert to grayscale\n",
    "        transforms.Resize(img_size_color),  # Resize to 84x84\n",
    "        transforms.ToTensor(),  # Convert to tensor, will have shape [1, 84, 84]\n",
    "    ])\n",
    "    return transform(frame).squeeze(0)  # Remove the single channel dimension, resulting in [84, 84]\n",
    "\n",
    "# Preprocess the frames (resize and convert to grayscale)\n",
    "def preprocess_frame_grey(frame):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Grayscale(),  # Convert to grayscale\n",
    "        transforms.Resize((256, 256)),  # Resize to 84x84\n",
    "        transforms.ToTensor(),  # Convert to tensor, will have shape [1, 84, 84]\n",
    "    ])\n",
    "    return transform(frame).squeeze(0)  # Remove the single channel dimension, resulting in [84, 84]\n",
    "\n",
    "def get_pointer(self, base_address, base_offset, offsets: list[int]):\n",
    "    # Calculate first dereferenced address\n",
    "    first_pointer = base_address + base_offset\n",
    "    dereferenced_address  = read_memory(self.process_handle, first_pointer, data_type=ctypes.c_uint64)\n",
    "    dereferenced_address_hex = (hex(dereferenced_address))\n",
    "\n",
    "    # Navigate through the other addresses to find the final pointer\n",
    "    for offset in offsets:\n",
    "        pointer = dereferenced_address + offset\n",
    "        dereferenced_address  = read_memory(self.process_handle, pointer)\n",
    "        dereferenced_address_hex = (hex(dereferenced_address))\n",
    "    return pointer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Replay-Buffer & Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size=10000):\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        state, action, reward, next_state, done = experience\n",
    "\n",
    "        # Ensure state and next_state are lists with image and cursor components\n",
    "        state_image, state_cursor_h, state_cursor_v = state\n",
    "        next_state_image, next_state_cursor_h, next_state_cursor_v = next_state\n",
    "\n",
    "        # Convert image components to numpy arrays for consistency\n",
    "        state_image = np.array(state_image)\n",
    "        next_state_image = np.array(next_state_image)\n",
    "\n",
    "        # Combine cursor components into arrays\n",
    "        state_cursor = np.array([state_cursor_h, state_cursor_v])\n",
    "        next_state_cursor = np.array([next_state_cursor_h, next_state_cursor_v])\n",
    "\n",
    "        # Add the full experience to the memory\n",
    "        self.memory.append(((state_image, state_cursor), action, reward, (next_state_image, next_state_cursor), done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # Randomly sample experiences from the memory\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the current size of the memory\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "\n",
    "class DQNAgent2:\n",
    "    def __init__(self, q_network: nn.Module, action_size: int):\n",
    "        self.action_size = action_size\n",
    "        self.memory = ReplayBuffer(buffer_size=10000)\n",
    "        self.gamma = 0.9  # Discount factor: How many steps back into the past is valuable\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 64\n",
    "        self.update_frequency = 4\n",
    "\n",
    "        # Create two networks: one for the current Q-function and one for the target Q-function\n",
    "        self.q_network = q_network\n",
    "        self.target_network = q_network.__class__(action_size)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.00001)\n",
    "        # Define the loss function\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        # Copy weights from the current network to the target network\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def select_action(self, state, explore=True):\n",
    "        # Unpack the state components\n",
    "        image, cursor_h, cursor_v = state  # Assuming state is [image, cursor_H, cursor_V]\n",
    "        \n",
    "        # Convert image to a torch tensor and add a batch dimension\n",
    "        image_tensor = torch.FloatTensor(image).unsqueeze(0)  # Shape: [1, channels, height, width]\n",
    "        \n",
    "        # Combine cursor positions into a tensor and add a batch dimension\n",
    "        cursor_positions = torch.FloatTensor([[cursor_h, cursor_v]])  # Shape: [1, 2]\n",
    "        \n",
    "        if explore and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        with torch.no_grad():  # Turn off gradients during evaluation\n",
    "            q_values = self.q_network(image_tensor, cursor_positions)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def random_action(self):\n",
    "        return random.randrange(self.action_size)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Sample a batch from the replay memory\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "\n",
    "        # Unzip the batch (states, actions, rewards, next_states, dones)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Extract image data and cursor positions from the states and next states\n",
    "        state_images = np.array([s[0] for s in states])  # Get images from current states\n",
    "        state_cursors = np.array([s[1] for s in states])  # Get cursor positions from current states\n",
    "\n",
    "        next_state_images = np.array([ns[0] for ns in next_states])  # Get images from next states\n",
    "        next_state_cursors = np.array([ns[1] for ns in next_states])  # Get cursor positions from next states\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        state_images = torch.tensor(state_images, dtype=torch.float32)\n",
    "        state_cursors = torch.tensor(state_cursors, dtype=torch.float32)\n",
    "        next_state_images = torch.tensor(next_state_images, dtype=torch.float32)\n",
    "        next_state_cursors = torch.tensor(next_state_cursors, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Get current Q-values by passing both image and cursor data to the Q-network\n",
    "        q_values = self.q_network(state_images, state_cursors)  # state_images: [batch_size, channels, height, width]\n",
    "        # print(f'q_values: {q_values}')\n",
    "\n",
    "        # Step 1: Unsqueeze the actions tensor to add a new dimension at position 1\n",
    "        actions_expanded = actions.unsqueeze(1)  # Shape becomes [batch_size, 1]\n",
    "        # print(f'actions_expanded.shape: {actions_expanded.shape}')\n",
    "\n",
    "        # Step 2: Gather the Q-values for the selected actions using the expanded actions tensor\n",
    "        selected_q_values = q_values.gather(1, actions_expanded)  # Shape: [batch_size, 1]\n",
    "\n",
    "        # Step 3: Squeeze to remove the singleton dimension from the result\n",
    "        q_values_for_actions = selected_q_values.squeeze(1)  # Shape: [batch_size]\n",
    "\n",
    "        # Get target Q values by passing both image and cursor data of next states to the target network\n",
    "        next_q_values = self.target_network(next_state_images, next_state_cursors).max(1)[0]\n",
    "\n",
    "        # Compute the target Q values for the current batch\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        # Compute the loss between predicted Q values and target Q values\n",
    "        loss = self.loss_fn(q_values_for_actions, target_q_values.detach())\n",
    "\n",
    "        # Backpropagate and update the Q-network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TestCNN(nn.Module):\n",
    "    def __init__(self, num_actions=5):\n",
    "        super(TestCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers for processing the 32x32 RGB image\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)  # 32x32 -> 32x32\n",
    "        \n",
    "        # Fully connected layers for the flattened output from the convolutional layers\n",
    "        self.fc_conv = nn.Linear(16 * 32 * 32, 128)  # Flatten to 1D, 128 output features\n",
    "        \n",
    "        # Dense layer for the cursor position input\n",
    "        self.cursor_fc = nn.Linear(2, 32)  # Processing (x, y) coordinates\n",
    "        \n",
    "        # Combined layers (Updated input size to 32800)\n",
    "        self.fc_combined1 = nn.Linear(16416, 64)  # Corrected input size\n",
    "        self.fc_combined2 = nn.Linear(64, num_actions)  # Output layer for action probabilities\n",
    "\n",
    "    def forward(self, image, cursor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image: Tensor of shape (batch_size, 3, 32, 32) representing the game board.\n",
    "            cursor: Tensor of shape (batch_size, 2) representing the cursor position (x, y).\n",
    "        \n",
    "        Returns:\n",
    "            policy: Tensor of shape (batch_size, num_actions) representing the action probabilities.\n",
    "        \"\"\"\n",
    "        # Process the image through convolutional layers\n",
    "        x = F.relu(self.conv1(image))   # Conv Layer 1\n",
    "        x = x.view(x.size(0), -1)       # Flatten the output to 1D\n",
    "        # Process the cursor position through a dense layer\n",
    "        y = F.relu(self.cursor_fc(cursor))\n",
    "        # Combine the features from the image and cursor\n",
    "        combined = torch.cat((x, y), dim=1)\n",
    "        # Fully connected layers for the combined features\n",
    "        z = F.relu(self.fc_combined1(combined))\n",
    "        policy = F.softmax(self.fc_combined2(z), dim=1)  # Action probabilities with softmax\n",
    "        \n",
    "        return policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create everything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment succesfully initialized \n",
      "- cursor: (3, 0)\n",
      "- score: 30\n"
     ]
    }
   ],
   "source": [
    "env = PPLEnv(base_address=0x7FFD2EC20000, color_mode='color', screenshot_saving=True)\n",
    "deep_q_learning_model = TestCNN(num_actions=env.action_space.n)\n",
    "agent = DQNAgent2(deep_q_learning_model, action_size=env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment succesfully initialized \n",
      "- cursor: (4, 10)\n",
      "- score: 0\n"
     ]
    }
   ],
   "source": [
    "env = PPLEnv(base_address=0x7FFD2EC20000, color_mode='color')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-2\n",
      "-3\n",
      "-4\n",
      "-5\n",
      "-6\n",
      "-7\n",
      "-8\n",
      "-9\n",
      "-10\n",
      "-11\n",
      "-12\n",
      "-13\n",
      "-14\n",
      "-15\n",
      "-16\n",
      "-17\n",
      "-18\n",
      "-19\n",
      "-20\n",
      "-21\n",
      "-22\n",
      "-23\n",
      "-24\n",
      "-25\n",
      "-26\n",
      "-27\n",
      "-28\n",
      "-29\n",
      "-30\n",
      "-31\n",
      "-32\n",
      "-33\n",
      "-34\n",
      "-35\n",
      "-36\n",
      "-37\n",
      "-38\n",
      "-39\n",
      "-40\n",
      "-41\n",
      "-42\n",
      "-43\n",
      "-44\n",
      "-45\n",
      "-46\n",
      "-47\n",
      "-48\n",
      "-49\n",
      "-50\n",
      "-51\n",
      "-52\n",
      "-53\n",
      "-54\n",
      "-55\n",
      "-56\n",
      "-57\n",
      "-58\n",
      "-59\n",
      "-60\n",
      "-61\n",
      "-62\n",
      "-63\n",
      "-64\n",
      "-65\n",
      "-66\n",
      "-67\n",
      "-68\n",
      "-69\n",
      "-70\n",
      "-71\n",
      "-72\n",
      "-73\n",
      "-74\n",
      "-75\n",
      "-76\n",
      "-77\n",
      "-78\n",
      "-79\n",
      "-80\n",
      "-81\n",
      "-82\n",
      "-83\n",
      "-84\n",
      "-85\n",
      "-86\n",
      "-87\n",
      "-88\n",
      "-89\n",
      "-90\n",
      "-91\n",
      "-92\n",
      "-93\n",
      "-94\n",
      "-95\n",
      "-96\n",
      "-97\n",
      "-98\n",
      "-99\n",
      "-100\n",
      "-101\n",
      "-102\n",
      "-103\n",
      "-104\n",
      "-105\n",
      "-106\n",
      "-107\n",
      "-108\n",
      "-109\n",
      "-110\n",
      "-111\n",
      "-112\n",
      "-113\n",
      "-114\n",
      "-115\n",
      "-116\n",
      "-117\n",
      "-118\n",
      "-119\n",
      "-120\n",
      "-121\n",
      "-122\n",
      "-123\n",
      "-124\n",
      "-125\n",
      "-126\n",
      "-127\n",
      "-128\n",
      "-129\n",
      "-130\n",
      "-131\n",
      "-250\n",
      "Episode 0/1, Reward: 0, Epsilon: 0.4598\n",
      "Training complete and model saved!\n"
     ]
    }
   ],
   "source": [
    "from modules.helper import alt_tab\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 1            # Total number of episodes to train\n",
    "max_steps_per_episode = 500    # Maximum steps per episode\n",
    "update_target_frequency = 10   # Update target network every 10 episodes\n",
    "epsilon_decay_factor = 0.995   # Decay rate for epsilon\n",
    "agent.q_network.train()\n",
    "agent.target_network.train()\n",
    "\n",
    "alt_tab()\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()  # Reset the environment at the start of each episode\n",
    "    state_image, cursor_h, cursor_v = state\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "\n",
    "        # Select an action (explore or exploit)\n",
    "        action = agent.select_action((state_image, cursor_h, cursor_v), explore=True)\n",
    "\n",
    "        # Take the action in the environment\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_image, next_cursor_h, next_cursor_v = next_state\n",
    "        print(reward)\n",
    "\n",
    "        # Store experience in the replay buffer\n",
    "        agent.memory.add(((state_image, cursor_h, cursor_v), action, reward, \n",
    "                          (next_image, next_cursor_h, next_cursor_v), done))\n",
    "\n",
    "        # Update the current state\n",
    "        state_image, cursor_h, cursor_v = next_image, next_cursor_h, next_cursor_v\n",
    "\n",
    "        # Perform replay and train the Q-network\n",
    "        agent.replay()\n",
    "\n",
    "        # If the episode ends, break the loop\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon\n",
    "    agent.epsilon = max(agent.epsilon_min, agent.epsilon * epsilon_decay_factor)\n",
    "\n",
    "    # Update the target network periodically\n",
    "    if episode % update_target_frequency == 0:\n",
    "        agent.update_target_network()\n",
    "\n",
    "    print(f\"Episode {episode}/{num_episodes}, Reward: {env.episode_score}, Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "alt_tab()\n",
    "torch.save(agent.q_network.state_dict(), \"trained_model_newest.pth\")\n",
    "print(\"Training complete and model saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/1, Reward: 0, Epsilon: 0.5967\n",
      "Testing completed!\n"
     ]
    }
   ],
   "source": [
    "from modules.helper import alt_tab\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 1            # Total number of episodes to train\n",
    "max_steps_per_episode = 500    # Maximum steps per episode\n",
    "update_target_frequency = 10   # Update target network every 10 episodes\n",
    "epsilon_decay_factor = 0.995   # Decay rate for epsilon\n",
    "agent.q_network.eval()\n",
    "agent.target_network.eval()\n",
    "\n",
    "alt_tab()\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()  # Reset the environment at the start of each episode\n",
    "    state_image, cursor_h, cursor_v = state\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "\n",
    "        # Select an action (explore or exploit)\n",
    "        action = agent.select_action((state_image, cursor_h, cursor_v), explore=False)\n",
    "\n",
    "        # Take the action in the environment\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_image, next_cursor_h, next_cursor_v = next_state\n",
    "\n",
    "        # Update the current state\n",
    "        state_image, cursor_h, cursor_v = next_image, next_cursor_h, next_cursor_v\n",
    "\n",
    "        # If the episode ends, break the loop\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update the target network periodically\n",
    "    if episode % update_target_frequency == 0:\n",
    "        agent.update_target_network()\n",
    "\n",
    "    print(f\"Episode {episode}/{num_episodes}, Reward: {env.episode_score}, Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "alt_tab()\n",
    "print(\"Testing completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
